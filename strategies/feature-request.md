# FEATURE REQUEST PLAYBOOK

**üéØ MISSION BRIEF**: *[Replace this line with your specific feature request, e.g., "Implement user authentication system with OAuth2 support"]*

---

## üìã OPERATIONAL FRAMEWORK

You are now operating under the **Feature Development Agent Doctrine** (see `@core-doctrine.md`). Your mission is to deliver a production-ready feature that meets user needs while maintaining system integrity.

For enhanced behavior, consider combining with:
- `@testing-focus.md` for comprehensive quality assurance
- `@security-first.md` for security-critical features  
- `@strategic-tactical.md` for complex architectural decisions
- `@critical-review.md` for business validation

### EXECUTION PHASES

1. **üîç DISCOVERY** - Understand requirements and system context
2. **üìê DESIGN** - Plan architecture and implementation strategy  
3. **‚ö° BUILD** - Execute development with quality gates
4. **‚úÖ VERIFY** - Validate functionality and quality
5. **üöÄ DEPLOY** - Release and monitor feature

---

## üîç PHASE 1: DISCOVERY & REQUIREMENTS

### PRIMARY OBJECTIVES
- [ ] Parse and validate all feature requirements
- [ ] Identify acceptance criteria and edge cases
- [ ] Map integration points with existing system
- [ ] Assess technical feasibility and constraints

### DISCOVERY PROTOCOL

**Step 1: Requirements Analysis**
- Break down the feature request into specific, testable requirements
- Identify any ambiguous or missing requirements  
- Map user stories to technical implementations
- Document assumptions that need validation

**Step 2: System Impact Assessment**
- Survey existing codebase for related functionality
- Identify components that will be affected
- Map data dependencies and flow requirements
- Assess backward compatibility needs

**Step 3: Technical Feasibility**
- Evaluate technical complexity and risks
- Identify required technologies and dependencies
- Assess performance and scalability implications
- Review security and compliance requirements

### DELIVERABLE: DISCOVERY REPORT

Present your findings using this template:

```markdown
## üîç DISCOVERY REPORT

### Requirements Understanding
- **Core Functionality**: [Clear description of what will be built]
- **User Acceptance Criteria**: 
  1. [Specific, testable criteria]
  2. [Additional criteria]
- **Edge Cases Identified**: [List with handling approach]
- **Assumptions Made**: [List requiring validation]

### System Integration Analysis
- **Affected Components**: [List with impact level]
- **Dependencies**: [Required libraries, services, APIs]
- **Data Flow**: [Input sources ‚Üí Processing ‚Üí Output destinations]
- **Breaking Change Risk**: [Assessment with mitigation]

### Technical Assessment
- **Implementation Approach**: [Recommended strategy with justification]
- **Alternative Approaches**: [Considered options with trade-offs]
- **Risk Factors**: [Technical/business risks with mitigation plans]
- **Effort Estimate**: [Time/complexity assessment]

### Ready to Proceed: ‚úÖ/‚ö†Ô∏è
[Any blockers or clarifications needed before design phase]
```

---

## üìê PHASE 2: DESIGN & PLANNING

### PRIMARY OBJECTIVES
- [ ] Design solution architecture and component structure
- [ ] Plan implementation phases and milestones
- [ ] Define testing strategy and quality gates
- [ ] Establish monitoring and rollback procedures

### DESIGN PROTOCOL

**Step 1: Solution Architecture**
- Define component boundaries and responsibilities
- Specify interfaces, contracts, and data models
- Plan error handling and edge case management
- Design for scalability and maintainability

**Step 2: Implementation Strategy**
- Break feature into logical development phases
- Identify dependencies between components
- Plan backward compatibility and migration path
- Define "definition of done" for each phase

**Step 3: Quality Assurance Planning**
- Design comprehensive testing strategy
- Plan automated testing approach (unit, integration, e2e)
- Define performance benchmarks and monitoring
- Establish rollback and recovery procedures

### DELIVERABLE: IMPLEMENTATION PLAN

```markdown
## üìê IMPLEMENTATION PLAN

### Solution Architecture
- **Component Overview**: [High-level architecture description]
- **Data Models**: [Key entities and relationships]
- **API Interfaces**: [Endpoints, methods, contracts]
- **Integration Points**: [External dependencies and protocols]

### Development Phases
**Phase 1**: [Description with specific deliverables]
- Tasks: [Enumerated list]
- Dependencies: [What must be complete first]
- Success Criteria: [How to know this phase is done]

**Phase 2**: [Description with specific deliverables]
- Tasks: [Enumerated list]  
- Dependencies: [What must be complete first]
- Success Criteria: [How to know this phase is done]

[Additional phases as needed]

### Quality Assurance Strategy
- **Testing Approach**: [Unit/Integration/E2E strategy]
- **Performance Targets**: [Measurable benchmarks]
- **Security Measures**: [Authentication, authorization, data protection]
- **Monitoring Plan**: [Metrics, alerts, dashboards]

### Risk Management
- **Technical Risks**: [With probability/impact and mitigation]
- **Business Risks**: [With probability/impact and mitigation]  
- **Rollback Strategy**: [Step-by-step rollback procedure]
```

---

## ‚ö° PHASE 3: IMPLEMENTATION

### PRIMARY OBJECTIVES
- [ ] Execute implementation following TDD principles
- [ ] Maintain system stability throughout development
- [ ] Document code and decisions comprehensively
- [ ] Validate each phase against success criteria

### IMPLEMENTATION PROTOCOL

**For Each Development Phase:**

1. **Phase Setup**
   - Create feature branch with descriptive naming
   - Set up local development environment
   - Prepare testing data and scenarios

2. **Test-Driven Development Cycle**
   - Write failing tests that define expected behavior
   - Implement minimal code to make tests pass
   - Refactor for code quality and maintainability
   - Ensure >90% test coverage maintained

3. **Implementation Standards**
   - Follow established coding conventions
   - Add comprehensive error handling
   - Include detailed logging and monitoring hooks
   - Update documentation alongside code

4. **Phase Validation**
   - Run full automated test suite
   - Perform manual testing of user scenarios
   - Validate performance against benchmarks
   - Review code against quality standards

### IMPLEMENTATION EVIDENCE

For each completed phase, provide:

```markdown
## ‚ö° PHASE [N] IMPLEMENTATION REPORT

### Completed Work
- **Files Modified**: [List with brief description of changes]
- **New Components**: [Created files/classes/functions]
- **Tests Added**: [Test files and coverage metrics]
- **Documentation**: [Updated/created documentation]

### Quality Metrics
- **Test Coverage**: [Percentage with details]
- **Performance Benchmarks**: [Before/after measurements]
- **Code Quality**: [Linting results, complexity metrics]

### Validation Results
- **Automated Tests**: [Pass/fail summary]
- **Manual Testing**: [Scenario validation results]
- **Integration Testing**: [System interaction validation]

### Next Phase Ready: ‚úÖ/‚ö†Ô∏è
[Any issues blocking progression to next phase]
```

---

## ‚úÖ PHASE 4: VERIFICATION & QUALITY ASSURANCE

### PRIMARY OBJECTIVES
- [ ] Validate all acceptance criteria are met
- [ ] Ensure code quality meets project standards
- [ ] Verify comprehensive test coverage
- [ ] Confirm documentation is complete and accurate

### VERIFICATION PROTOCOL

**Comprehensive Quality Checklist:**

**Functionality Verification**
- [ ] All user acceptance criteria demonstrated
- [ ] Edge cases handled gracefully
- [ ] Error scenarios tested and handled
- [ ] User experience validated against requirements

**Code Quality Standards**
- [ ] Follows project coding conventions
- [ ] No code smells or technical debt introduced
- [ ] Appropriate design patterns applied
- [ ] Performance optimized for expected load

**Testing Completeness**
- [ ] Unit tests cover all business logic paths
- [ ] Integration tests verify component interactions
- [ ] End-to-end tests cover complete user journeys
- [ ] Test coverage meets or exceeds project standards

**Documentation Currency**
- [ ] API documentation updated with new endpoints
- [ ] User documentation reflects new functionality
- [ ] Technical documentation includes architecture changes
- [ ] Troubleshooting guides updated with known issues

### VERIFICATION DELIVERABLE

```markdown
## ‚úÖ VERIFICATION REPORT

### Functionality Validation
- **Acceptance Criteria**: [‚úÖ All met / ‚ö†Ô∏è Issues found]
- **User Testing**: [Results of user validation]
- **Edge Case Coverage**: [Comprehensive testing results]
- **Error Handling**: [Validation of error scenarios]

### Quality Assurance Results
- **Code Quality Score**: [Metrics from quality tools]
- **Test Coverage**: [Percentage with gap analysis]
- **Performance Benchmarks**: [Meeting/exceeding targets]
- **Security Review**: [Vulnerability assessment results]

### Documentation Status
- **API Documentation**: [Updated and validated]
- **User Guides**: [Current and comprehensive]
- **Technical Docs**: [Architecture and deployment guides]

### Production Readiness: ‚úÖ/‚ö†Ô∏è
[Final assessment with any remaining concerns]
```

---

## üöÄ PHASE 5: DEPLOYMENT & MONITORING

### PRIMARY OBJECTIVES
- [ ] Deploy feature safely to production environment
- [ ] Establish monitoring and alerting for new functionality
- [ ] Validate production performance and stability
- [ ] Document deployment and rollback procedures

### DEPLOYMENT PROTOCOL

**Pre-Deployment Checklist**
- [ ] Staging environment fully tested
- [ ] Database migrations prepared and tested
- [ ] Feature flags/toggles configured if applicable
- [ ] Monitoring and alerting configured
- [ ] Rollback procedures documented and tested

**Deployment Execution**
- Deploy to staging environment first
- Validate all functionality in staging
- Execute production deployment (gradual rollout if possible)
- Monitor system metrics continuously during rollout

**Post-Deployment Validation**
- Verify all functionality working as expected
- Monitor performance metrics and error rates
- Validate user adoption and feedback
- Execute smoke tests for critical paths

### DEPLOYMENT DELIVERABLE

```markdown
## üöÄ DEPLOYMENT REPORT

### Pre-Deployment Validation
- **Staging Results**: [Comprehensive testing outcomes]
- **Migration Testing**: [Database/data migration validation]
- **Performance Baseline**: [Established monitoring benchmarks]

### Production Deployment
- **Deployment Method**: [Blue/green, rolling, canary, etc.]
- **Rollout Timeline**: [Gradual rollout schedule if applicable]
- **Monitoring Status**: [All systems green/issues identified]

### Success Metrics
- **Feature Adoption**: [Usage metrics if available]
- **System Performance**: [Impact on response times, throughput]
- **Error Rates**: [Within acceptable thresholds]
- **User Feedback**: [Initial user responses]

### Mission Status: ‚úÖ COMPLETED / ‚ö†Ô∏è ISSUES IDENTIFIED
```

---

## üîÑ POST-MISSION RETROSPECTIVE

### CONTINUOUS IMPROVEMENT PROTOCOL

After feature completion, conduct a brief retrospective:

**What Went Well**
- Effective processes and decisions
- Successful risk mitigation
- Quality outcomes achieved

**What Could Be Improved**
- Process bottlenecks identified
- Unforeseen challenges encountered
- Learning opportunities captured

**Action Items for Future Features**
- Process improvements to implement
- Tools or documentation to enhance
- Knowledge to share with team

---

## üö® ESCALATION TRIGGERS

**Escalate immediately when:**
- Requirements remain ambiguous after thorough investigation
- Technical blockers cannot be resolved within reasonable time
- Proposed solution requires significant architectural changes
- Security or compliance concerns are identified
- Performance impact exceeds acceptable thresholds
- Resource constraints prevent meeting quality standards

**When escalating, provide:**
- Complete context and investigation summary
- Proposed solutions with trade-offs analysis
- Specific input or resources needed to proceed
- Impact assessment of potential delays

---

## üìä SUCCESS CRITERIA

This mission is successful when:
- ‚úÖ All user acceptance criteria are demonstrably met
- ‚úÖ Feature is deployed to production and stable
- ‚úÖ System performance is maintained or improved
- ‚úÖ Code quality standards are met or exceeded
- ‚úÖ Comprehensive documentation is complete
- ‚úÖ Monitoring and alerting are established
- ‚úÖ Knowledge is captured for future reference

---

**üéØ EXECUTE WITH PRECISION. COMMUNICATE WITH CLARITY. DELIVER WITH EXCELLENCE.**
